import os 
import sys
import tqdm
import json
from datasets import load_dataset

# To use the latest Megatron-LM and the NeMo repos instead of the ones in the container. Clone the repos to these paths
sys.path.insert(1, '/workspace/development_code/Megatron-LM')
sys.path.insert(1, '/workspace/development_code/NeMo')
dir_to_write = os.getcwd() # provide the path to directory the save the output .jsonl file
select = [24] # Set to the category of the gigaspeech subset  


def create_dataset(data, output_file):
    instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'
    to_json = []
    k = 0 # counter save to file every 50 runs

    for datapoint in data:
        
        audio_path = datapoint['audio']['path']
        _ = datapoint.pop('audio')
        
        # String normailization : convert to lowercase and replace  <QUESTIONMARK>, <PERIOD>, <COMMA>
        ground_truth = datapoint['text'].replace(' <QUESTIONMARK>','?').replace(' <PERIOD>','.').replace(' <COMMA>',',').lower()
        
        # setting number of candidates to generate with temperature sampling 
        topk = 200
        
        paths = []
        for i in range(topk):
            paths.append(audio_path)
            
        transcipts = asr_model.transcribe(paths, return_hypotheses=True, batch_size = topk)
        
        # removing redundant transcripts
        unique = []
        for i in transcipts[1]:
            unique.append(i.text)
        unique = set(unique)
        
        print(f'{len(set(unique))} uniques candidates')
        
        # selecting 15 candidates 
        for_input = '\n'.join(list(unique)[:15])
        
        prompt_with_no_response = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\n{instruction}\n\n### Input:\n{for_input}\n\n### Response:"""
        to_json.append( {**datapoint, 'output':ground_truth, 'input': prompt_with_no_response}) 

        k = k + 1
        
        if k % 50 == 0:
            
            print(f'Compleated {k} candidates and writing to json file')
            with open(output_file, 'w') as f:
                for o in to_json:
                    f.write(json.dumps(o)+"\n")


# Extracting and cleaning the dataset from HuggingFace
data = load_dataset("speechcolab/gigaspeech", "xs", token = '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', cache_dir = '/workspace/development_code/hf_data_cache_dir')

# Code to print number of audio clips in each category 
scs = []
for i in tqdm.tqdm(data['train'])    :
    scs.append(i['category'])
    
scategories = set(scs.copy())
    
sdic = {}
for c in scategories:
    sdic[c]=0
for c in scs:
    sdic[c] = sdic[c]+1
print(sdic)

index = ["People and Blogs", "Business", "Nonprofits and Activism", "Crime", "History", "Pets and Animals", "News and Politics", "Travel and Events", "Kids and Family", "Leisure", "N/A", "Comedy", "News and Politics", "Sports", "Arts", "Science and Technology", "Autos and Vehicles", "Science and Technology", "People and Blogs", "Music", "Society and Culture", "Education", "Howto and Style", "Film and Animation", "Gaming", "Entertainment", "Travel and Events", "Health and Fitness", "audiobook"]
for i in sdic.keys():
    no = sdic[i]
    cls = index[i]
    print(f"{i} {cls}: {no}")

# Here we chose the Gaming category (index 24) before
train_data =[]

for i in tqdm.tqdm(data['train']):
    if i['category'] in select:
        train_data.append(i)
        
# splitting the dataset into train and test (80/20 split) since dev and val set do not have category information
split = int(len(train_data)*.8)
test_data = train_data[split:]
train_data = train_data[:split]

#Generating N-Best Hypothesis with Nemo ASR backend
# Loading NeMo ASR model 
import NeMo.nemo.collections.asr as nemo_asr
asr_model = nemo_asr.models.ASRModel.from_pretrained("stt_en_fastconformer_transducer_large")

# creating the jsonl files for LLaMA 2 model below
print('Create train dataset')
create_dataset(train_data, os.path.join(dir_to_write,f"cat{select[0]}_train.jsonl"))

print('Create test dataset')
create_dataset(test_data, os.path.join(dir_to_write,f"cat{select[0]}_test.jsonl"))